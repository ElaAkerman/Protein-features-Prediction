{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "def save_predictions(predictions, filename):\n",
    "    df = pd.DataFrame(np.array(predictions).T,\n",
    "                      columns=['attribute 1', 'attribute 2', 'attribute 3', 'attribute 4', 'attribute 5'])\n",
    "    df.to_csv(filename, index_label='Id')\n",
    "\n",
    "\n",
    "X_test = np.genfromtxt('protein_features_test.csv', delimiter=',')\n",
    "x_train = pd.read_csv('protein_features_train.csv', header=None)\n",
    "y_train = pd.read_csv('protein_labels_train.csv', header=None)\n",
    "\n",
    "# create a vector of labels for each class\n",
    "labels_train = [np.copy(y_train.iloc[:, i]) for i in range(0, y_train.shape[1])]\n",
    "\n",
    "# Getting the data ready in order for us to split each label to a different model.\n",
    "x_train1 = x_train.copy()\n",
    "x_train2 = x_train.copy()\n",
    "x_train3 = x_train.copy()\n",
    "x_train4 = x_train.copy()\n",
    "x_train5 = x_train.copy()\n",
    "\n",
    "X_test1 = X_test.copy()\n",
    "X_test2 = X_test.copy()\n",
    "X_test3 = X_test.copy()\n",
    "X_test4 = X_test.copy()\n",
    "X_test5 = X_test.copy()\n",
    "\n",
    "y_train1 = y_train.iloc[:, 0]\n",
    "y_train2 = y_train.iloc[:, 1]\n",
    "y_train3 = y_train.iloc[:, 2]\n",
    "y_train4 = y_train.iloc[:, 3]\n",
    "y_train5 = y_train.iloc[:, 4]\n",
    "\n",
    "\n",
    "xgb1 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9, colsample_bynode=1,\n",
    "                     colsample_bytree=0.7, gamma=0, gpu_id=-1, importance_type='gain',\n",
    "                     interaction_constraints='',\n",
    "                     learning_rate=0.07, max_delta_step=0, max_depth=11,\n",
    "                     min_child_weight=0.09,\n",
    "                     monotone_constraints='()', n_estimators=760, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
    "                     reg_alpha=0.2, reg_lambda=0.3, scale_pos_weight=1, subsample=1.0,\n",
    "                     tree_method='exact',\n",
    "                     use_label_encoder=False, validate_parameters=1, verbosity=0)\n",
    "xgb2 = XGBClassifier(learning_rate=0.01, n_estimators=2720, max_depth=20, min_child_weight=1, gamma=0, subsample=0.6,\n",
    "                     colsample_bytree=0.7, colsample_bylevel=0.7,\n",
    "                     reg_alpha=0.005, objective='binary:logistic', nthread=4, scale_pos_weight=5, seed=24, n_jobs=-1)\n",
    "xgb3 = XGBClassifier(n_estimators=1500, learning_rate=0.01, subsample=0.6, max_depth=17, n_thread=4, n_jobs=-1)\n",
    "xgb4 = XGBClassifier(learning_rate=0.01, n_estimators=2720, max_depth=20, min_child_weight=1, gamma=0, subsample=0.6,\n",
    "                     colsample_bytree=0.7, colsample_bylevel=0.7,\n",
    "                     reg_alpha=0.005, objective='binary:logistic', nthread=4, scale_pos_weight=5, seed=24, n_jobs=-1)\n",
    "xgb5 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
    "                     enable_categorical=False,\n",
    "                     gamma=0.0, gpu_id=-1, importance_type=None, interaction_constraints='', learning_rate=0.01,\n",
    "                     max_delta_step=0,\n",
    "                     max_depth=20, min_child_weight=1, monotone_constraints='()', n_estimators=940, n_jobs=4, nthread=4,\n",
    "                     num_parallel_tree=1, predictor='auto', random_state=27, reg_alpha=0.01, reg_lambda=1,\n",
    "                     scale_pos_weight=1, seed=27, subsample=0.9, tree_method='exact', validate_parameters=1,\n",
    "                     verbosity=None)\n",
    "\n",
    "model_1 = xgb1.fit(x_train1, y_train1)\n",
    "model_2 = xgb2.fit(x_train2, y_train2)\n",
    "model_3 = xgb3.fit(x_train3, y_train3)\n",
    "model_4 = xgb4.fit(x_train4, y_train4)\n",
    "model_5 = xgb5.fit(x_train5, y_train5)\n",
    "\n",
    "models_list = [model_1, model_2, model_3, model_4, model_5]\n",
    "\n",
    "prediction_1 = model_1.predict_proba(X_test1)[:, 1]\n",
    "prediction_2 = model_2.predict_proba(X_test2)[:, 1]\n",
    "prediction_3 = model_3.predict_proba(X_test3)[:, 1]\n",
    "prediction_4 = model_4.predict_proba(X_test4)[:, 1]\n",
    "prediction_5 = model_5.predict_proba(X_test5)[:, 1]\n",
    "predictions_list = [prediction_1, prediction_2, prediction_3, prediction_4, prediction_5]\n",
    "\n",
    "save_predictions(predictions_list, 'elaakerman 4.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
